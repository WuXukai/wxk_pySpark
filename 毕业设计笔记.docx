毕业设计笔记

修改host映射
sudo vim /etc/hosts

安装ssh无密码登陆
cd ~
	ssh-keygen -t rsa  一路回车
	cd ~/.ssh
	cat id_rsa.pub >> authorized_keys
	chmod 600 authorized_keys

jdk1.8
tar -zxvf jdk-8u231-linux-x64.tar.gz  -C ~/app/

配置环境变量
vim ~/.bashrc (ubuntu vim ~/.bashrc)
export JAVA_HOME=/home/wxk/app/jdk1.8.0_231
export PATH=$JAVA_HOME/bin:$PATH
source ~/.bashrc(ubuntu source ~/.profile)

scala2.11.8
tar -zxvf scala-2.11.8.tgz  -C ~/app
配置环境变量
vim ~/.bashrc
export SCALA_HOME=/home/wxk/app/scala-2.11.8
export PATH=$SCALA_HOME/bin:$PATH
source ~/.bashrc

maven3.3.9
tar -zxvf apache-maven-3.3.9-bin.tar.gz -C ~/app/
配置环境变量
vim ~/.bashrc
export MAVEN_HOME=/home/wxk/app/apache-maven-3.3.9
export PATH=$MAVEN_HOME/bin:$PATH
source ~/.bashrc

修改maven配置
mkdir ~/maven_repository
vim $MAVEN_HOME/conf/settings.xml
  <localRepository>/home/wxk/maven_repository</localRepository>

添加maven阿里云仓库
在setttins.xml文件中找到<mirrors></mirrors>标签对,进行修改：

<mirrors>
      <mirror>
         <id>nexus-aliyun</id>
         <mirrorOf>*</mirrorOf>
         <name>Nexus aliyun</name>
         <url>http://maven.aliyun.com/nexus/content/groups/public</url>
      </mirror> 
</mirrors>

安装python3.6.5
cd ~/software/
wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz
tar -zxvf Python-3.6.5.tgz
配置环境变量
--编译前安装依赖，python依赖安装

yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel

cd Python-3.6.5/

./configure --prefix=/home/wxk/app/python3

make && make install

cd /home/wxk/app/python3/bin
pwd
--配置环境变量
vi ~/.bashrc
export PATH=/home/wxk/app/python3/bin:$PATH
source ~/.bashrc




hadoop-2.6.0-cdh5.7.0
tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app/
配置环境变量
vim ~/.bashrc
export HADOOP_HOME=/home/wxk/app/hadoop-2.6.0-cdh5.7.0
export PATH=$HADOOP_HOME/bin:$PATH
source ~/.bashrc

cd /home/wxk/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop

配置hadoop
vim hadoop-env.sh
export JAVA_HOME=/home/wxk/app/jdk1.8.0_231
source ~/.bashrc

vim core-site.xml
<property>
    <name>fs.default.name</name>
    <value>hdfs://wxk000:8020</value>
</property>

mkdir ~/app/tmp
vim hdfs-site.xml 
<property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/wxk/app/tmp/dfs/name</value>
</property>

<property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/wxk/app/tmp/dfs/data</value>
</property>

<property>
    <name>dfs.replication</name>
    <value>1</value>
</property>

配置yarn
cp mapred-site.xml.template  mapred-site.xml
vim mapred-site.xml
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>

vim yarn-site.xml
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>

格式化hadoop
cd $HADOOP_HOME/bin
./hadoop namenode -format

启动HDFS：
		第一次执行的时候一定要格式化文件系统，不要重复执行: hdfs namenode -format
		启动集群：$HADOOP_HOME/sbin/start-dfs.sh   (这里记得先改好主机名映射为wxk000)
		验证:
			[hadoop@hadoop000 sbin]$ jps
			60002 DataNode
			60171 SecondaryNameNode
			59870 NameNode

			http://wxk000:50070
			如果发现jps ok，但是浏览器不OK？ 十有八九是防火墙问题
			查看防火墙状态：sudo firewall-cmd --state
			关闭防火墙: sudo systemctl stop firewalld.service
			进制防火墙开机启动：



hadoop软件包常见目录说明
	bin：hadoop客户端名单
	etc/hadoop：hadoop相关的配置文件存放目录
	sbin：启动hadoop相关进程的脚本
	share：常用例子


注意：
	start/stop-dfs.sh与hadoop-daemons.sh的关系
	start-dfs.sh = 
		hadoop-daemons.sh start namenode
		hadoop-daemons.sh start datanode
		hadoop-daemons.sh start secondarynamenode
	stop-dfs.sh = 
		....




HDFS命令行操作   *****
	shell-like
		mkdir  ls ....

hadoop fs [generic options]
	[-appendToFile <localsrc> ... <dst>]
	[-cat [-ignoreCrc] <src> ...]
	[-chgrp [-R] GROUP PATH...] 更改组
	[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
	[-chown [-R] [OWNER][:[GROUP]] PATH...]
	[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]
	[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
	[-count [-q] [-h] [-v] [-x] <path> ...]
	[-cp [-f] [-p | -p[topax]] <src> ... <dst>]
	[-df [-h] [<path> ...]]
	[-du [-s] [-h] [-x] <path> ...]
	[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
	[-getmerge [-nl] <src> <localdst>]
	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]
	[-mkdir [-p] <path> ...]
	[-moveFromLocal <localsrc> ... <dst>]
	[-moveToLocal <src> <localdst>]
	[-mv <src> ... <dst>]
	[-put [-f] [-p] [-l] <localsrc> ... <dst>]
	[-rm [-f] [-r|-R] [-skipTrash] <src> ...]
	[-rmdir [--ignore-fail-on-non-empty] <dir> ...]
	[-text [-ignoreCrc] <src> ...]

hadoop常用命令：
hadoop fs -ls /
hadoop fs -put
hadoop fs -copyFromLocal
hadoop fs -moveFromLocal
hadoop fs -cat
hadoop fs -text
hadoop fs -get
hadoop fs -mkdir 
hadoop fs -mv  移动/改名
hadoop fs -getmerge
hadoop fs -rm
hadoop fs -rmdir
hadoop fs -rm -r


HDFS存储扩展：
	put: 1file ==> 1...n block ==> 存放在不同的节点上的
	get: 去nn上查找这个file对应的元数据信息
	了解底层的存储机制这才是我们真正要学习的东西，掌握API那是毛毛雨



使用HDFS API的方式来操作HDFS文件系统
	pycharm 
	使用Maven3.3.9来管理项目


启动yarn：
$HADOOP_HOME/sbin/start-yarn.sh

http://wxk000:8088


spark
cd ~/source
tar -zxvf spark-2.3.0.tgz
export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"
cd spark-2.3.0/
编译spark
./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0
(cdh版本无法编译备用命令)
./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0

安装spark
tar -zxvf spark-2.3.0-bin-2.6.0-cdh5.7.0.tgz -C ~/app/
测试
~/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/bin/spark-shell
wxk000:4040

配置spark环境变量
vim ~/.bashrc
export PYSPARK_PYTHON=python3.6

export SPARK_HOME=/home/wxk/app/spark-2.3.0-bin-2.6.0-cdh5.7.0
export PATH=$SPARK_HOME/bin:$PATH
source ~/.bashrc



Spark Core核心RDD
编程入口:SparkContext

一、RDD是什么


官网：xxxx.apache.org
源码：https://github.com/apache/xxxx


abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
) extends Serializable with Logging

1）RDD是一个抽象类
2）带泛型的，可以支持多种类型： String、Person、User


RDD：Resilient Distributed Dataset   弹性 分布式 数据集

Represents an 
	immutable：不可变
	partitioned collection of elements ：分区
		Array(1,2,3,4,5,6,7,8,9,10)  3个分区： (1,2,3) (4,5,6) (7,8,9,10)
	that can be operated on in parallel： 并行计算的问题

单机存储/计算==>分布式存储/计算
1）数据的存储: 切割    HDFS的Block
2）数据的计算: 切割(分布式并行计算)   MapReduce/Spark
3）存储+计算 :   HDFS/S3+MapReduce/Spark
	
	==> OK 








